<style>
 .call-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100vh;
}

.call-message {
  font-size: 24px;
  font-weight: bold;
  text-align: center;
  margin-bottom: 20px;
}

.call-name {
  color: #007bff;
}

.call-audio {
  margin-top: 20px;
}
</style>

<div class="call-container">
  <p class="call-message">Ordering a pizza for <span class="call-name">{{name}}</span>.</p>
  <audio id="audio-element"></audio>
  <div class="d-flex">
    <button id="start-stream" class="btn btn-primary">Listen</button>
    <button id="end-call" class="btn btn-danger">End Call</button>
  </div>
</div>

<script src="https://cdn.jsdelivr.net/npm/wavefile"></script>

<script>

// const audio = new Audio();
// var audio = document.querySelector("#audio-element");

function playBuffer(bufferArray) {

    let wav = new wavefile.WaveFile();
    wav.fromScratch(2, 8000, '8m', bufferArray);
		wav.fromMuLaw()
		wav.toSampleRate(44100)

    let wavDataURI = wav.toDataURI();

    let audio = new Audio(wavDataURI);
    audio.play();
}


function dequeueAudio(mediaQueue){

	var media = mediaQueue.shift()

	// If the queue's first two items have the same timestamp (or off by just a few ms), then merge them
	var isSimilarTimestamp = Math.abs(media.timestamp - mediaQueue[0].timestamp) <= 5

	// Get both payloads
	if (isSimilarTimestamp){
		
		if(media.track == 'inbound'){
			media.inboundPayload = media.payload
			media.outboundPayload = mediaQueue.shift().payload
		}
		else{
			media.inboundPayload = mediaQueue.shift().payload
			media.outboundPayload = media.payload
		}

		media.track = "merged"
	}

	// Else one payload will have silence
	else{
		
		// Return the first item in the queue, merged with silence
		if(media.track == 'inbound'){
			media.inboundPayload = media.payload
			media.outboundPayload = "/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////w==" 
			// var outboundPayload = Buffer.from(zeros).toString('base64')
		}
		else{
			// var inboundPayload = Buffer.from(zeros).toString('base64')
			media.inboundPayload = "/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////w==" 
			media.outboundPayload = media.payload
		}

	}

	return media

}

function concatUint8Arrays(a, b) {
  const c = new Uint8Array(a.length + b.length);
  c.set(a);
  c.set(b, a.length);
  return c;
}

function base64ToArrayBuffer(base64) {
  var binaryString = window.atob(base64);
  var binaryLen = binaryString.length;
  var bytes = new Uint8Array(binaryLen);
  for (var i = 0; i < binaryLen; i++) {
      var ascii = binaryString.charCodeAt(i);
      bytes[i] = ascii;
  }
  return bytes;
}

const socket = new WebSocket('ws://localhost:3000/audio_stream');
socket.binaryType = 'arraybuffer'; // Ensure that we receive binary data
var mediaQueue = []

document.getElementById('start-stream').addEventListener('click', function() {
  socket.addEventListener('message', function(event) {  

    const media = JSON.parse(event.data);
    {{!-- playBuffer([base64ToArrayBuffer(media.payload), base64ToArrayBuffer(media.payload)]) --}}
    {{!-- return --}}
    
    // Add the audio data to the queue for recording
    mediaQueue.push(media)

    if(mediaQueue.length < 5){
      return
    }
  
    // Sort the buffer by timestamp
    mediaQueue.sort((a, b) => {
      return parseInt(a.timestamp) - parseInt(b.timestamp);
    });

    // Send next media chunk to client
    var processedMedia = dequeueAudio(mediaQueue)

    // console.log(processedMedia)

    console.log("inbound: " + processedMedia.inboundPayload)
    console.log("outbound: " + processedMedia.inboundPayload)
    console.log("")

    var inboundBinary = base64ToArrayBuffer(processedMedia.inboundPayload)
    var outboundBinary = base64ToArrayBuffer(processedMedia.outboundPayload)

    playBuffer([inboundBinary, outboundBinary])
  })
})

{{!-- 
// When the user starts to stream the audio
document.getElementById('start-stream').addEventListener('click', function() {

  // twilio sends audio data as 160 byte messages containing 20ms of audio each
  // we will buffer 20 twilio messages corresponding to 0.4 seconds of audio to improve throughput performance
  const BUFFER_SIZE = 20 * 160;
  // the algorithm to deal with mixing the two channels is somewhat complex
  // here we implement an algorithm which fills in silence for channels if that channel is either
  //   A) not currently streaming (e.g. the outbound channel when the inbound channel starts ringing it)
  //   B) packets are dropped (this happens, and sometimes the timestamps which come back for subsequent packets are not aligned)
  let inbuffer = new Uint8Array();
  let outbuffer = new Uint8Array();
  let inbound_chunks_started = false;
  let outbound_chunks_started = false;
  let latest_inbound_timestamp = 0;
  let latest_outbound_timestamp = 0;


  
  const socket = new WebSocket('ws://localhost:3000/audio_stream');
  socket.binaryType = 'arraybuffer'; // Ensure that we receive binary data


  socket.addEventListener('message', function(event) {

    const media = JSON.parse(event.data);
    const chunk = base64ToArrayBuffer(media['payload']);
    if (media['track'] == 'inbound') {
      // fills in silence if there have been dropped packets
      if (inbound_chunks_started) {
        if (latest_inbound_timestamp + 20 < parseInt(media['timestamp'])) {
          const bytes_to_fill = 8 * (parseInt(media['timestamp']) - (latest_inbound_timestamp + 20));
          // NOTE: 0xff is silence for mulaw audio
          // and there are 8 bytes per ms of data for our format (8 bit, 8000 Hz)
          inbuffer = concatUint8Arrays(inbuffer, new Uint8Array(bytes_to_fill).fill(0xff));
        }
      } else {
        // make it known that inbound chunks have started arriving
        inbound_chunks_started = true;
        latest_inbound_timestamp = parseInt(media['timestamp']);
        // this basically sets the starting point for outbound timestamps
        latest_outbound_timestamp = parseInt(media['timestamp']) - 20;
      }
      latest_inbound_timestamp = parseInt(media['timestamp']);
      // extend the inbound audio buffer with data
      inbuffer = concatUint8Arrays(inbuffer, new Uint8Array(chunk));
    }
    if (media['track'] == 'outbound') {
      // make it known that outbound chunks have started arriving
      outbound_chunked_started = true;
      // fills in silence if there have been dropped packets
      if (latest_outbound_timestamp + 20 < parseInt(media['timestamp'])) {
        const bytes_to_fill = 8 * (parseInt(media['timestamp']) - (latest_outbound_timestamp + 20));
        // NOTE: 0xff is silence for mulaw audio
        // and there are 8 bytes per ms of data for our format (8 bit, 8000 Hz)
        outbuffer = concatUint8Arrays(outbuffer, new Uint8Array(bytes_to_fill).fill(0xff));
      }
      latest_outbound_timestamp = parseInt(media['timestamp']);
      // extend the outbound audio buffer with data
      outbuffer = concatUint8Arrays(outbuffer, new Uint8Array(chunk));
    }

    // check if our buffer is ready to play
    while (inbuffer.length >= BUFFER_SIZE && outbuffer.length >= BUFFER_SIZE) {
      playBuffer([inbuffer.slice(0, BUFFER_SIZE), outbuffer.slice(0, BUFFER_SIZE)])

      // clearing buffers
      inbuffer = inbuffer.slice(BUFFER_SIZE);
      outbuffer = outbuffer.slice(BUFFER_SIZE);
    }

  }) 
});
--}}
</script>